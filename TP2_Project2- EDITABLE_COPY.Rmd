---
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyfoot[CO,CE]{Hair Parra, Alessio Bessan, Ioan Catalin}
  - \fancyfoot[LE,RO]{\thepage}
title: "TP2 Risk Management"
author: 'TP2: Hair Parra , Alessio Bressan, Ioan Catalin'
date: "`r Sys.Date()`"
geometry: margin=1.3cm
output: 
    pdf_document: 
      extra_dependencies: ["array", "amsmath","booktabs"]
---

<!--These are definitions of Latex Environments--> 
\newtheorem{assumption}{Assumption}[assumption]
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark*}{Remark}
\newtheorem{exercise*}{Exercise}


```{r setup, include=FALSE}
# additional setup options
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height=6) 

# configurations for plot 
my_plot_hook <- function(x, options)
  paste("\n", knitr::hook_plot_tex(x, options), "\n")
knitr::knit_hooks$set(plot = my_plot_hook)

# numeric format 
options("scipen" = 10)
```

### Libraries 

```{r, message=FALSE, echo=FALSE}
#Note needed to install plotly library for the surface
#install.packages(plotly)
# Preload R libraries we will use 
library(here)
library(xts) 
library(zoo)
library(quantmod)
library(plotly)

# additional source code
# source(here("code", "NormalCopulaPdf.R")) # compute the pdf of a normal copula
# source(here("code", "StudentCopulaPdf.R")) # compute the pdf of a normal copula
# source(here("code", "DisplayCopula.R")) # display the pdf through a 3-d chart

# additional source code for this assg
source(here("code", "Utils.R")) # display the pdf through a 3-d chart
source(here("code", "OptionPricing.R")) # BlackScholes and Option pricing
source(here("code", "GARCH.R")) # display the pdf through a 3-d chart
```


## Risk Management: European Options Portfolio

The objective is to implement (part of) the risk management framework for estimating the risk of a book of European call options by taking into account the risk drivers such as underlying and implied volatility.

# Data 

Load the database Market. Identify the price of the **SP500**, the **VIX index**, the term structure of interest rates (current and past), and the traded options (calls and puts).

```{r}
# load dataset into environment
load(file = here("data_raw", "Market.rda"))

# reassign name and inspect structure of loaded data 
mkt <- Market 
summary(mkt)
```

```{r}
str(mkt)
```

Let's unpack these into the env. individually: 

```{r}
# unpack each of the elements in the mkt list
sp500 <- mkt$sp500
vix <- mkt$vix 
Rf <- mkt$rf # risk-free rates
calls <- mkt$calls 
puts <- mkt$puts 

# assign colname for aesthetic
colnames(sp500) <- "sp500"
colnames(vix) <- "vix"
```

**SP500 and VIX** 

By inspection, we observe that we the SP500 and VIX indices are contained in the `sp500` and `vix` xts objects respectively. 

```{r}
# show head of both indexes 
head(sp500) 
head(vix)
```

```{r fig.height=8, fig.width=8, fig.align='center'}
par(mfrow = c(2,1)) 

# plot both series on top of each other 
plot(sp500) 
plot(vix)
```


**Interest Rates** 

The **interest rates** are given in the `$rf` attribute. We can see that 

```{r}
Rf
```
These represent the interest rates at different maturities. The maturities are given as follows:

```{r}
r_f <- as.vector(Rf)
names(r_f) <- c("1d","1w", "1m", "3m", "6m", "9m", "1y", "2y", "3y", "4y", "5y", "7y","10y", "30y")
r_f
```

Further, we can pack different sources of information in a matrix: 

```{r}
# pack Rf into a matrix with rf, years, and days
rf_mat <- as.matrix(r_f)
rf_mat <- cbind(rf_mat, as.numeric(names(Rf)))
rf_mat <- cbind(rf_mat, rf_mat[, 2]*360)
colnames(rf_mat) <- c("rf", "years", "days") 
rf_mat
```

```{r, echo=FALSE}
# turn into dataframe
rf_df <- data.frame(rf_mat)
rf_df$years2 <- rf_df$years**2 
rf_df$years3 <- rf_df$years**3

# perform linear regresison 
rf_lm <- lm(rf ~ 1 + years + years2 + years3, data = rf_df)

# obtain predictions
preds <- predict(rf_lm)

# extract index 
x <- rf_df$years
ix  <- sort(x, index.return=T)$ix

# 
plot(x, rf_df$rf, 
     main="Term Structure of Risk-Free Rates", 
     xlab = "Years", 
     ylab="Rf", 
     col="blue",
     type="p",
     pch=16, 
     cex=1.2
     )

# add polynomial curve to plot 
lines(x[ix], preds[ix], col='black', lwd=2)
grid()
```


**Calls** 

The `calls` object displays the different values of $K$ (**Strike Price**), $\tau$ (**time to maturity**) and $\sigma = IV$ (**Implied Volatilty**)

```{r}
dim(calls)
head(calls)
```
Add `days` column for convenience: 

```{r}
calls <- cbind(calls, calls[, "tau"]*250)
colnames(calls) <- c("K","tau", "IV", "tau_days")
head(calls)
tail(calls)
```

**Puts** 

```{r}
dim(puts)
head(puts)
```

```{r}
puts <- cbind(puts, puts[, "tau"]*250)
colnames(puts) <- c("K","tau", "IV", "tau_days")
head(puts)
tail(puts)
```

**Definining the book of options**

```{r}
# Initialize strikes and maturities the options 
T_vec <- c(20, 20, 40,40) #  maturities 
K_vec <- c(1600, 1650, 1750, 1800) # Strikes 
```


\newpage
# Pricing a Portfolio of Options 

## Black-Scholes 

Notation: 

- $S_t$ = Current value of underlying asset price
- $K$ = Options **strike price** 
- $T$ = Option **maturity** (in years) 
- $t$ = **time** in years
- $\tau$ = $T-t$ = **Time to maturity** 
- $r$ = **Risk-free rate** 
- $y$ **Dividend yield**
- $R$ = $r-y$ 
- $\sigma$ = **Implied volatility** 
- $c$ = **Price Call Option** 
- $p$ = **Price Put Option** 

\begin{proposition}[Black-Scholes Model] 
Assume the notation before, and let $N(\cdot)$ be the cumulative standard normal distribution function. Under certain assumptions, the Black-Scholes models prices Call and Put options as follows: 
$$
\begin{cases}
C(S_t, t) = Se^{yT} N(d_1) - Ke^{-r \times \tau} N(d_2), \\ 
\; \\
P(S_t, t) = Ke^{-r \times \tau}(1 - N(d_2))  - Se^{y \times T}(1-N(d_1)), 
\end{cases}
$$

where: 
$$
\begin{cases} 
d_1 = \dfrac{\ln\left( \dfrac{S_t}{K} \right) + \tau\left( r + \dfrac{\sigma^2}{2}  \right)}{\sigma \sqrt{\tau}} \\ 
d_2 = d_1 - \sigma \sqrt{\tau} \\ 
\end{cases}
$$
, further the Put Option price corresponds to the **Put-Call parity**, given by: 

$$
C(S_t, t) + Ke^{-r \times \tau} = P(S_t, t) + S_{t}
$$

\end{proposition}

**Note** As here we don't have dividends, then $y=0$, and so 

$$
\begin{cases}
C(S_t, t) = S_tN(d_1) - Ke^{-r \times \tau} N(d_2), \\ 
\; \\
P(S_t, t) = Ke^{-r \times \tau}(1 - N(d_2))  - S_t(1-N(d_1)), 
\end{cases}
$$

### BlackScholes function 

```{r}
# source code with options pricign
source(here("code", "OptionPricing.R")) # BlackScholes and Option pricing

# Test: Call Option 
S_t = 1540
K = 1600 
r = 0.03
tau = 10/360 
sigma = 1.05
black_scholes(S_t, K, r, tau, sigma)
```

## Book of Options

Assume the following book of **European Call Options:** 

1. **1x** strike $K=1600$ with maturity $T=20d$ 
2. **1x** strike $K=1650$ with maturity $T=20d$
3. **1x** strike $K=1750$ with maturity $T=40d$ 
2. **1x** strike $K=1800$ with maturity $T=40d$

Find the price of this book given **the last underlying price** and the **last implied volatility** (take the VIX for all options). Use **Black-Scholes** to price the options. Take the current term structure and **linearly interpolate** to find the corresponding rates. Use 360 days/year for the term structure and **250 days/year** for the maturity of the options. 

### Nearest values 

This function will obtain the two nearest values $a, b$ for a number $x$ in a vector $v$, such that $a<x<b$. 

```{r}
# Test: function used to get two nearest values in a vector (OptionsPricing.R)
days <- rf_mat[, "days"]
get_nearest(40, rf_mat[, "days"]) # nearest day values 
```

### Linear Interpolation

Given two known values $(x_1, y_1)$ and $(x_2, y_2)$, we can estimate the $y$-value for some $x$-value with:

$$
y = y_1 + \dfrac{(x-x_1)(y_2-y_1)}{(x_2-x_1)}
$$

```{r}
# Function to interpolate y given two points
interpolate <- function(x,x1=1,y1=1,x2=2,y2=2){
  y1 + (x-x1)*(y2-y1)/(x2-x1)
}
```


### Finding the rates through interpolation 

The **yield curve** for the given structure of interest rates can be modeled a function $r_f = f(x)$ ,where $x$ is the number of years. Then, we can interapolate the values from `rf_mat`. This is done in the `price_option()` function under `code/OptionPricing.R`

**Example** 

ex.: **1x** strike $K=1600$ with maturity $T=20d$ 

```{r}
S_t = sp500[length(sp500)] # last price of underlying 
IV = vix[length(vix)] # last volatility 

## test: specific price (func from OptionPricing.R)
price_option(T=20, K=1600, calls = calls, rf_mat =  rf_mat, stock = NA, S_t = S_t, IV = IV)
```

Next, using the function above we price the book of options given: 

1. **1x** strike $K=1600$ with maturity $T=20d$ 
2. **1x** strike $K=1650$ with maturity $T=20d$
3. **1x** strike $K=1750$ with maturity $T=40d$ 
2. **1x** strike $K=1800$ with maturity $T=40d$

First, we retrieve the latest value for the underlying (SP500) and the latest implied volatility (VIX): 

```{r}
S_t = sp500[length(sp500)] # last price of underlying 
IV = vix[length(vix)] # last volatility 
```

Then, we price the options accordingly: 
 
```{r}
# First Call Option 
price_option(T=20, K=1600, calls=calls, rf_mat=rf_mat, S_t = S_t, IV = IV)
```

```{r}
# Second Call Option 
price_option(T=20, K=1650, calls=calls, rf_mat=rf_mat, S_t = S_t, IV = IV)
```

```{r}
# Third Call Option 
price_option(T=40, K=1750, calls=calls, rf_mat=rf_mat, S_t = S_t, IV = IV)
```

```{r}
# Fourth Call Option 
price_option(T=40, K=1800, calls=calls, rf_mat=rf_mat, S_t = S_t, IV = IV)
```
\newpage
# One risk driver and Gaussian Model 

### Log-returns 

The **discrete returns** are given by: 

$$
R_{t+1} = \dfrac{P_{t+1} - P_{t}}{P_{t}}
$$

and the next ahead log-returns are given by: 

$$
\log(R_{t+1}) = \log(P_{t+1} - P_{t}) - \log(P_{t})
$$

```{r, message=FALSE}
# load reqruired libraries 
library("PerformanceAnalytics")

# calculate returns
sp500_rets <- PerformanceAnalytics::CalculateReturns(sp500, method="log")
vix_rets <- PerformanceAnalytics::CalculateReturns(vix, method="log")

# remove first return 
sp500_rets <- sp500_rets[-1] 
vix_rets <- vix_rets[-1]

# remove nas 
sp500_rets[is.na(sp500_rets)] <- 0
vix_rets[is.na(vix_rets)] <- 0

# display
head(sp500_rets) 
head(vix_rets)
```
\newpage

#One risk driver and Gaussian model
Steps:
1. Compute the daily log-returns of the underlying stock.
2. Assume they are iid normally distributed.
3. Generate 10 000 scenarios for the one-week ahead (five days) underlying price using the normal
distribution fitted to the past invariants.
4. Determine the P&L distribution of the book of options, using the simulated underlying values. Assume
the implied volatility stays the same. Take interpolated rates for the term structure.
5. Compute the VaR95 and the ES95.



Generating the 10 000 scenarios for the one-week ahead (five days) underlying price using the normal
distribution fitted to the past invariants
```{r}
mean_sp500 = mean(sp500_rets) #mean of sp500
sd_sp500 = sd(sp500_rets) #standard deviation of sp500



#initialize matrix of returns forecasted until T+5
sp500_rets_forecast = matrix(NA,10000,5)

#simulating 10k returns using the mean and the std dev from our sp500 returns
for(n_ahead in 1:5)
{
  sp500_rets_forecast[1:10000, n_ahead] = rnorm(10000,
                                                mean = mean_sp500,
                                                sd = sd_sp500)
}

colnames(sp500_rets_forecast) = c("T+1", "T+2", "T+3", "T+4", "T+5")

head(sp500_rets_forecast)
```



Determining the P&L distribution of the book of options, using the simulated underlying values
```{r}
#initialize matrix of prices forecasted until T+5
sp500_price_forecast = matrix(NA,10000,5)


p_prev = sp500[length(sp500)][[1]] #previous price (before T+1)

#converting returns to prices
for(i in 1:5)
{
  sp500_price_forecast[,i] = f_next_Pt(p_prev,
                                       sp500_rets_forecast[,i])
  p0 = sp500_rets_forecast[,i]
}
colnames(sp500_price_forecast) =  c("T+1", "T+2", "T+3", "T+4", "T+5")
head(sp500_price_forecast)
```


```{r}
T = c(20,20,40,40)  #Time to maturity of our book of options

K = c(1600,1650,1750,1800)  #Strike price of our book of options

#Matrix of call prices initialisation
call_price_matrices <- lapply(rep(1, 4), # generate three empty matrices of compatible sizes
                              function(x){
                                matrix(NA, 
                                       nrow(sp500_price_forecast), 
                                       ncol(sp500_price_forecast),
                                       dimnames=list(seq(1:nrow(sp500_price_forecast)),
                                                     c("T+1", "T+2", "T+3", "T+4", "T+5")
                                       )
                                )
                              }
)

#matrices of P&L initialization
PL_matrices = call_price_matrices


#Loop to calculate the P&L of each option from our book of options
for(i in 1:4)
{
  for(j in 1:5)
  {
    price_call = prc_opt(T[i]-j,K[i], calls, rf_mat, sp500_price_forecast[,j], vix[[length(vix)]])
    call_price_matrices[[i]][,j] = price_call
    PL_matrices[[i]][,j] = option_profit(S = sp500_price_forecast[,j], K = K[i],
                                         c = price_call)$call_profit
  }
}

#showing the values of the P&L distribution for the first option where T = 20 and K = 1600
PL_matrices[[1]][1:5,1:5]
```

Computing the VaR95 and the ES95 for our book of options
```{r}
opt1_VaR_ES = f_VaR_ES(PL_matrices[[1]], alpha = 0.05)  #VaR95 and ES95 option 1 of our book of options
opt2_VaR_ES = f_VaR_ES(PL_matrices[[2]], alpha = 0.05)  #VaR95 and ES95 option 2 of our book of options
opt3_VaR_ES = f_VaR_ES(PL_matrices[[3]], alpha = 0.05)  #VaR95 and ES95 option 3 of our book of options
opt4_VaR_ES = f_VaR_ES(PL_matrices[[4]], alpha = 0.05)  #VaR95 and ES95 option 4 of our book of options
```


```{r}
opt1_VaR_ES#Showing the VaR95 and ES95 option 1 of our book of options
```


```{r}
opt2_VaR_ES#Showing the VaR95 and ES95 option 2 of our book of options
```


```{r}
opt3_VaR_ES#Showing the VaR95 and ES95 option 3 of our book of options
```


```{r}
opt4_VaR_ES#Showing the VaR95 and ES95 option 4 of our book of options
```

\newpage
# Two risk drivers and Gaussian Model 

1. Compute the daily log-returns of the underlying stock.
2. Compute the daily log-returns of the VIX.
3. Assume they are invariants normally distributed.
4. Generate 10 000 scenarios for the one-week ahead underlying price and the one week ahead VIX value
   using the normal distribution fitted to the past risk drivers.
5. Determine the P&L distribution of the book of options, using the simulated values. Take interpolated
   rates for the term structure.

```{r}
#####################################################
###  ---> INSERT YOUR PART FROM HERE (ALESSIO) <---- ###
############################################## ######
rets <- cbind(sp500_rets, vix_rets)

#Number of days ahead
n_day <- 5

#Assume that the distribution is normally fitted
mu <- apply(rets, 2, mean)

Sigma <- cov(rets)

n_sim <- 10000

#memory allocation
updated_rets <- matrix(NA, n_sim, 2)
rets_day <- matrix(NA, n_sim)

#iterations
set.seed(1234)
for(i in 1:n_day){
    updated_rets <- rmvnorm(mean = mu, sigma = Sigma, n = n_sim)
    
    if(i == 1){
      rets_day <- updated_rets
    }
    else{
      rets_day <- cbind(rets_day,updated_rets)
    }
    
}
head(rets_day)

colnames(rets_day) <- c("Day 1", "Day 1",
                        "Day 2", "Day 2",
                        "Day 3", "Day 3",
                        "Day 4", "Day 4",
                        "Day 5", "Day 5")


daily_rets <- list(SP500 = rets_day[,c(1, 3, 5, 7, 9)],
                   VIX = rets_day[, c(2, 4, 6, 8, 10)])

head(daily_rets$SP500)

```

Converting the daily log returns to the value of the underlying
```{r}
#Defining an initial value

init <- as.numeric(c(Market$sp500[length(Market$sp500)],
              Market$vix[length(Market$vix)]))

prices <- f_logret_to_price(sp_init = init[1],vix_init = init[2], sim_rets_sp500 = daily_rets$SP500,
                            sim_rets_vix = daily_rets$VIX, n_day)

###################################
#####ONLY USE THIS TEMPORARILY#####
###################################

names(prices) <- c("SP500", "VIX")
```


Pricing the book of options 
```{r}
#Book of options
#T_vec = c(20, 20, 40, 40)
#K_vec = c(1600, 1650, 1750, 1800)

#memory allocation
option_prices <- lapply(rep(1, length(K_vec)), # generate three empty matrices of compatible sizes
                  function(x){
                    matrix(NA, 
                           nrow(prices$SP500), 
                           ncol(prices$SP500),
                           dimnames=list(seq(1:nrow(prices$SP500)),
                                         c("T+1", "T+2", "T+3", "T+4", "T+5")
                                         )
                         )
                    }
                  )
names(option_prices) <- c("K1", "K2", "K3", "K4")
#loop through and calculated the PL
for(i in 1:n_day){
  #Underlying price at day i
  price_t <- prices$SP500[,i]
  
  #Underlying volatility at day i
  vol_t <- prices$VIX[,i]
  
  #calculating the price of the option
  for(j in 1:4){
    option_prices[[j]][,i] <- prc_opt(T_vec[j]-i, K_vec[j], calls, rf_mat, price_t, vol_t)
  }
}

head(option_prices$K1)

##################################
#### OPTION PRICES INCORRECT #####
##################################

#Need to fix the function f_logret_to_prices

```
Calculating the profitability of each option

```{r}

#Profit of an option -> PL = max(S-K,0)-price
PL_matrices_Two_Risk <- lapply(rep(1, 4), #memory allocation
                  function(x){
                    matrix(NA, 
                           nrow(prices$SP500), 
                           ncol(prices$SP500),
                           dimnames=list(seq(1:nrow(prices$SP500)),
                                         c("T+1", "T+2", "T+3", "T+4", "T+5")
                                         )
                           )
                    }
                  )
names(PL_matrices_Two_Risk) <- c("PL1", "PL2", "PL3", "PL4")

for(i in 1:n_day){
  #spot price of underlying at day i
  spot <- prices$SP500[,i]
  
  for(j in 1:length(K_vec)){
    premium <- option_prices[[j]][,i]
    PL_matrices_Two_Risk[[j]][,i] <- max((spot - K_vec[j]), 0) - premium
  }
}

head(PL_matrices_Two_Risk$PL1)
```
Graph the PL to find the distribution

```{r fig.height=9, fig.width=11, fig.align='center'}
#window modification
par(mfrow = c(2,2))

#Distribution P&L for Call option 1
hist(PL_matrices_Two_Risk$PL1, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K[1], " T=", T[1], " (Call)"))
lines(density(PL_matrices_Two_Risk$PL1), lwd=2, col="blue")
rug(PL_matrices_Two_Risk$PL1)

#Distribution P&L for Call option 2
hist(PL_matrices_Two_Risk$PL2, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K[1], " T=", T[1], " (Call)"))
lines(density(PL_matrices_Two_Risk$PL2), lwd=2, col="blue")
rug(PL_matrices_Two_Risk$PL2)

#Distribution P&L for Call option 3
hist(PL_matrices_Two_Risk$PL3, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K[1], " T=", T[1], " (Call)"))
lines(density(PL_matrices_Two_Risk$PL3), lwd=2, col="blue")
rug(PL_matrices_Two_Risk$PL3)

#Distribution P&L for Call option 4
hist(PL_matrices_Two_Risk$PL4, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K[1], " T=", T[1], " (Call)"))
lines(density(PL_matrices_Two_Risk$PL4), lwd=2, col="blue")
rug(PL_matrices_Two_Risk$PL4)


```


\newpage
## Two risk drivers and copula-marginal model (Student-t and Gaussian Copula) 

1. Compute the daily log-returns of the underlying stock 
2. Assume the first invariant is generated using a Student-t distribution with $\nu=10$ df and the second invariant is generated using a Student-t distribution with $\nu=5$ df.
3. Assume the **normal copula** to merge the marginals. 
4. Generate 10000 scenarios for the one-week ahead price for the underlying and the one-week ahead VIX value using the copula. 
5. Determine the P&L distribution of the book of options, using the simulated values. 
6. Take interpolated rates for the term structure.  

### Gaussian Copula with two Student-t marginals

A bivariate distribution $H$ can be formed via a copula $C$ from two marginal distributions with CDFs $F$ and $G$ via:

$$
H(x,y) = C(F(x), G(y)) = C(F^{-1}(u), G^{-1}(u))
$$

with density 

$$
h(x,y) = c(F(x), G(y))f(x)g(y)
$$

The **Gaussian Copula** is given by: 

$$
C^{\text{Gauss}}_{\rho}(u,v) = \Phi_{\rho}(\Phi^{-1}(u), \Phi^{-1}(v)).
$$

In this case, a Gaussian copula with two Student-t marginals with CDFs $t(\nu_1)$ with $\nu_1$ degrees of freedom and $t(\nu_2)$ with $\nu_2$ degrees of freedom is given by:

$$
C^{\text{Gauss}}_{\rho}(u,v) = \Phi_{\rho}(F_{\nu_1}^{-1}(u), F_{\nu_1}^{-1}(v)), 
$$
where $F_{\nu_1}$ and $F_{\nu_2}$ are their respective CDFs. 

### Generating the simulation scenarios 

Assumptions: 
- Marginal Student-t distributions 
- Disregard time dependence in the bootstrapping process 


```{r, warning=FALSE}
# Load required libraries 
library("fGarch")
library("MASS")
library("copula")
library("Matrix")

# random seed for replication 
set.seed(123)

# convert to vector since fitting without dependence 
sp500_rets_vec <- as.vector(sp500_rets) 
vix_rets_vec <- as.vector(vix_rets)

# calculate means and sds for both indices 
mu <- c(mean(sp500_rets_vec), mean(vix_rets_vec)) 
sigma <- c(sd(sp500_rets_vec), sd(vix_rets_vec)) 

# display
mu
sigma
```

```{r}
## Fit marginals by MLE

# Student-t for sp500
fit1 <- suppressWarnings(
  fitdistr(x = sp500_rets_vec, 
           densfun = dstd,
           start = list(mean = 0, sd = 1, nu = 10))
  )
theta1 <- fit1$estimate #extract fitted parameters

# Student-t for vix
fit2 <- suppressWarnings(
  fitdistr(x = vix_rets_vec, 
           densfun = dstd,
           start = list(mean = 0, sd = 1, nu = 5))
  )
theta2 <- fit2$estimate # extract fitted parameters

# display parameters
theta1
theta2
```


```{r}
# Fit Student-t to the marginals
# U1 <- pstd(sp500_rets_vec, mean = theta1[1], sd = theta1[2], nu = theta1[3]) # sp500
# U2 <- pstd(vix_rets_vec,mean = theta2[1], sd = theta2[2], nu = theta2[3]) # vix
U1 <- pstd(sp500_rets_vec, mean = theta1[1], sd = theta1[2], nu = 10) # sp500
U2 <- pstd(vix_rets_vec,mean = theta2[1], sd = theta2[2], nu = 5) # vix
# U1 <- pt(sp500_rets_vec, df = 5) # sp500
# U2 <- pt(vix_rets_vec, df = 10) # vix
U <- cbind(U1, U2) # join into one matrix
plot(U, pch = 20, cex = 0.9)
```


```{r}
# Obtain the best rho for the Gaussian Copula
C <- normalCopula(dim = 2)
fit <- fitCopula(C, data = U, method = "ml")
fit
```


```{r}
# seed for replication
set.seed(420)

# Simulation parameters
n_sim = 10000 # set number of simulations
# n_ahead = 5 # days ahead to produce samples

# produce simulations from copula
U_sim <- rCopula(n_sim, fit@copula)

# use copula U_sim to reproduce the marginals with student-t distr
# rets1_sim <- qstd(U_sim[,1], mean = mu[1], sd = sigma[1], nu = theta1[3]) # sp500
# rets2_sim <- qstd(U_sim[,2], mean = mu[1], sd = sigma[1], nu = theta2[3]) # vix
rets1_sim <- qstd(U_sim[,1], mean = mu[1], sd = sigma[1], nu = 10) # sp500
rets2_sim <- qstd(U_sim[,2], mean = mu[1], sd = sigma[1], nu = 5) # vix
rets_sim <- cbind(rets1_sim, rets2_sim)

# visualize
par(mfrow = c(2,2)) 
hist(rets1_sim, nclass=50)
hist(rets2_sim, nclass=50)
hist(rets_sim, nclass = round(10 * log(n_sim)))
```


```{r}
# random seed for replication 
set.seed(69)

##############################
### Setup & Initialization ###
##############################

# Simulation parameters
n_sim = 10000 # set number of simulations
n_ahead = 5 # days ahead to produce samples 

# preallocate matrices to store simulations
sim_rets_sp500 <- matrix(NA, nrow = n_sim, ncol=5)
sim_rets_vix <- matrix(NA, nrow = n_sim, ncol=5)

# assign days ahead 
colnames(sim_rets_sp500) <- c("T+1", "T+2", "T+3", "T+4", "T+5")
colnames(sim_rets_vix) <- c("T+1", "T+2", "T+3", "T+4", "T+5")

##############################
### Running the simulation ###
##############################

# perform n_head days of n_sim scenarios
for(t in 1:n_ahead){

  # Sample 5-days ahead from Gaussian Copula
  U_sim <- rCopula(n_sim, fit@copula)
  
  # use copula U_sim to reproduce the marginals quantiles F^{-1}(u) with student-t distr
  rets1_sim <- qstd(U_sim[,1], mean = theta1[1], sd = theta1[2], nu = 10) # sp500
  rets2_sim <- qstd(U_sim[,2], mean = theta2[1], sd = theta2[2], nu = 5) # vix
  # rets1_sim <- qt(U_sim[,1], df = 10) # sp500
  # rets2_sim <- qt(U_sim[,2], df = 5) # vix
  rets_sim <- cbind(rets1_sim, rets2_sim)
  
  # store simulation of log return in matrix
  sim_rets_sp500[ ,t] <- rets1_sim
  sim_rets_vix[ ,t] <- rets2_sim
}

# preview of simulated log returns
head(sim_rets_sp500)
head(sim_rets_vix)
```

### Computing Prices from Returns 

Next, we crate a function to forecast the 5 day ahead prices from the returns. Since: 


$$
\begin{aligned}
R_{t} = \dfrac{P_{t} - P_{t-1}}{P_{t-1}} \\
\implies R_{t} = \dfrac{P_{t}}{P_{t-1}} - 1 \\ 
\implies \log(R_{t}) = \log\left( \dfrac{P_{t}}{P_{t-1}} \right) \\ 
\implies \log(R_{t}) = \log(P_{t}) - \log(P_{t-1}) \\ 
\implies \log(P_{t}) = \log(R_{t})+ \log(P_{t-1}) \\ 
\implies P_{t} = \exp(\log(R_{t})+ \log(P_{t-1}) ) \\ 
\implies P_{t+1} = \exp(\log(R_{t+1})+ \log(P_{t}) )
\end{aligned}
$$
This logic is implemented in the `f_logret_to_price()` function through the `f_next_Pt()` function, under the `code/Utils.R` script. 


```{r}
# Obtain Initial values (last value of simulation) 
spT <- sp500[length(sp500)][[1]]
vixT <- vix[length(vix)][[1]]

# calculate the price and values from the simulated log-returns 
sim_val_mats <- f_logret_to_price(sp_init = spT,
                                  vix_init = vixT, 
                                  sim_rets_sp500 = sim_rets_sp500, 
                                  sim_rets_vix = sim_rets_vix,
                                  n_day)

# unpack matrices 
sim_price_sp500 <- sim_val_mats$sp500 
sim_vol_vix <- sim_val_mats$vix
```


```{r}
# compare simulated returns with the price
head(sim_rets_sp500)
head(sim_price_sp500) 
```

```{r}
# compare simualted log rets with volatility 
head(sim_rets_vix)
head(sim_vol_vix)
```

### Pricing the simulation scenarios 

Recall the initial (call) options: 

1. **1x** strike $K=1600$ with maturity $T=20d$ 
2. **1x** strike $K=1605$ with maturity $T=40d$
3. **1x** strike $K=1800$ with maturity $T=40d$ 

**Option Pricing of Simulated Values**

Next, we calculate the price of the book of options for the simulated values using the `f_opt_price_simulation()` function under `code/OptionPricing.R`: 

```{r}
# random seed for replication 
set.seed(123)

# Obtain the obtion prices from simulation values 
opt_price_mats <- f_opt_price_simulation(sim_price_sp500 = sim_price_sp500, 
                                         sim_vol_vix = sim_vol_vix, 
                                         K_vec = K_vec, 
                                         T_vec = T_vec, 
                                         put=FALSE)

```



```{r}
# overview of dataframes
head(opt_price_mats$opt1)
head(opt_price_mats$opt2)
head(opt_price_mats$opt3)
head(opt_price_mats$opt4)
```

### Distribution of the Profit and Loss for the Book Of Options 

Recall the profit functions for European options: 

### Parameters 

**Parameters:** 
- $S$: Spot price (current)
- $S_0$: Spot price at the beginnin of the option
- $S_T$: Spot price at maturity
- $T$: Maturity of option
- $K$: Strike price
- $c$: Price of Call option
- $p$: Price of Put option


### Profit at Maturity

The profit functions of a long call and a long put are given by: 

$$
\begin{aligned}
\pi^{\text{Long Call}} = \max(S_T - K, 0) - c \\ 
\pi^{\text{Long Put}} = \max(K - S_T, 0) - p
\end{aligned}
$$


**Calculating the profits** 

For each of the simulated prices and resulting premiums, we want to calculate the profit generated at each simulation timestep. 
The function used is `f_pl_simulation()`, found under `code/OptionPricing.R`. 

```{r}
# Initialize strikes and maturities the options 
T_vec <- c(20, 20, 40,40) #  maturities 
K_vec <- c(1600, 1650, 1750, 1800) # Strikes 

# Compute the profits and loses for the simulation  from the simulated option premiums
PL_mats <- f_pl_simulation(sim_price_sp500 = sim_price_sp500, 
                           opt_price_mats = opt_price_mats, 
                           K_vec = K_vec)

```


```{r}
# display profit matrices 
head(PL_mats$PL1)
head(PL_mats$PL2)
head(PL_mats$PL3)
head(PL_mats$PL4)
```

**Distribution of Options P/L** 

Next, using all the simulated profits and losses for each of the options, we display a histogram for the distribution for each of the options, for the aggregated 5 days of simulation: 

```{r fig.height=9, fig.width=11, fig.align='center'}
# flatten the matrices 5-days ahead simulated P/L for the three options 
sim_pl_opt1 <- as.vector(PL_mats$PL1)
sim_pl_opt2 <- as.vector(PL_mats$PL2)
sim_pl_opt3 <- as.vector(PL_mats$PL3)
sim_pl_opt4 <- as.vector(PL_mats$PL4)

# Compute the 95% VaR and 95% ES
opt1_VaR_ES <- f_VaR_ES(sim_pl_opt1, alpha = 0.05)
opt2_VaR_ES <- f_VaR_ES(sim_pl_opt2, alpha = 0.05)
opt3_VaR_ES <- f_VaR_ES(sim_pl_opt3, alpha = 0.05)
opt4_VaR_ES <- f_VaR_ES(sim_pl_opt4, alpha = 0.05)

# plot the distribution for each of the options 
par(mfrow = c(2,2))
hist(sim_pl_opt1, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K_vec[1], " T=", T_vec[1], " (Call)"))
lines(density(sim_pl_opt1), lwd=2, col="blue")
abline(v=opt1_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt1_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt1)


hist(sim_pl_opt2, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K_vec[2], " T=", T_vec[2], " (Call)"))
lines(density(sim_pl_opt2), lwd=2, col="blue")
abline(v=opt2_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt2_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt2)


hist(sim_pl_opt3, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K_vec[3], " T=", T_vec[3], " (Call)"))
lines(density(sim_pl_opt3), lwd=2, col="blue")
abline(v=opt3_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt3_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt3)

hist(sim_pl_opt4, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K_vec[4], " T=", T_vec[4], " (Call)"))
lines(density(sim_pl_opt4), lwd=2, col="blue")
abline(v=opt4_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt4_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt4)
```

These all look like multimodal distributions. The last one, particularly shows a different mode for each of the fives days computed. The 95% VaR (red) and ES (black) are all displayed in the plots. 


### VaR95

**Definition** 

For a random variable $X$, the **Value-at-Risk (VaR)** at level $\alpha$ is defined as the $\alpha$-lower quantile of the distribution of X, thus: 
$$
VaR_X(\alpha) = F^{-1}_{X}(1 - \alpha)
$$

**First Option** 

```{r}
opt1_VaR_ES$VaR # first option 
opt2_VaR_ES$VaR # second doption 
opt3_VaR_ES$VaR # third option
opt4_VaR_ES$VaR # fourth option
```
## ES95 

Expected shortfall is calculated by averaging all of the returns in the distribution that are worse than the VAR of the portfolio at a given level of confidence.

```{r}
# display 
opt1_VaR_ES$ES
opt2_VaR_ES$ES
opt3_VaR_ES$ES
opt4_VaR_ES$ES
```


\newpage
# Volatility Surface 

1.Fit a volatility surface to the implied volatilities observed on the market (traded call and put options).
  Minimize the absolute distance between the market implied volatilities and the model implied volatilities.
  The parametric surface is given by:
               σ(m, τ ) = α1 + α2(m − 1)2 + α3(m − 1)3 + α4√τ 

2. Re-price the portfolio in one week assuming the same parametric model but shifted by the one-year
  ATM implied volatility difference.

Note: Need to install the library *plotly* to graph the volatility surface

```{r}
#########################################################
###  ---> INSERT YOUR PART FROM HERE (ALESSIO) <---- ###
############################################## #########
#initialize the last price of the underlying
S <- Market$sp500[length(Market$sp500)][[1]] #3410
VIX <- as.numeric(Market$vix[length(Market$vix)])

# convert to draframe for easier manipulation 
calls_df <- as.data.frame(calls)
puts_df <- as.data.frame(puts)

# assign extra column to puts (1) and calls (0)
calls_df["type"] <- "call"
puts_df["type"] <- "put"

# check dimensions 
dim(calls_df) 
dim(puts_df)

# stack both of these matrices together 
puts_calls <- rbind(calls_df, puts_df) 

# integrate the price 
puts_calls["S"] <- rep(S, nrow(puts_calls))
puts_calls["m"] <- puts_calls["K"]/puts_calls["S"]

# filter the calls that have moniness over one 
calls_m_over <- puts_calls[(puts_calls["type"] == "call") & (puts_calls["m"] >= 1), ]

# filter the puts that have moniness below one
puts_m_under <- puts_calls[(puts_calls["type"] == "put") & (puts_calls["m"] < 1), ]

# combine these results into putcalls again 
call_put_data <- rbind(calls_m_over, puts_m_under)

```
**Functions for the parametric surface fit**
```{r}
f_sig <- function(alpha, m, t){
  #m is an array of K/S
  #t is a repetitive array same tau for K
  #IV is a vector or numbers
  #alpha is a set of paramters
  a1 <- alpha[1]
  a2 <- alpha[2]
  a3 <- alpha[3]
  a4 <- alpha[4]
  
  sig <- a1 + a2*(m-1)^2 + a3*(m-1)^3 + a4*sqrt(t)
  
  return(sig)
}

f_fit <- function(alpha, m, t, IV){
  
  sig <- f_sig(alpha, m, t)
  error <- abs(IV - sig)
  
  return(sum(error))
}
```

Optimize the function

```{r}
#Initialization
start <- c(1, 1, 0, 1)

test <- f_fit(start, m = call_put_data$m[1:100],
                    t = call_put_data$tau[1:100],
                    IV = call_put_data$IV[1:100])

#Optimize
fit <- optim(par = start,
             fn = f_fit,
             method = "BFGS",
             m = call_put_data$m,
             t = call_put_data$tau,
             IV = call_put_data$IV)

#optimal parameters
alpha <- fit$par

```

Volatility Surface Plot

```{r}
#Summary of data
Moneyness <- call_put_data$m
Tau <- call_put_data$tau

summary(Moneyness)
summary(Tau)

#Initialize x and y axis
maturity <- seq(from =0, to = 2.5, by = 0.005)
moneyness <-  seq(from =0, to = 2, by = 0.005)

#memory allocation
IV <- matrix(NA, nrow = length(moneyness), ncol = length(maturity))

#Creating a surface 
for(i in 1:length(moneyness)){
  for(j in 1:length(maturity)){
    IV[i,j] <- f_sig(alpha, moneyness[i], maturity[j])
  }
}
#Try plotting now
plot_ly(x =~ moneyness, y =~ maturity, z =~ IV) %>% add_surface() 

```
2. Re-price the portfolio in one week assuming the same parametric model but shifted by the VIX difference

There is a distance between the model and the data, we need to keep the same distance and project it forward

```{r}
#Simulate the foward path for the VIX at the 5 day horizon assuming both normal distributions
load(here("data_out","sim_vol_vix_student_copula.rda"))

#Assuming we take the mean of the 5th simulated day
VIX_sim <- colMeans(sim_vol_vix)[5]

#Finding the initial difference between model and ATM
#delta of the VIX T = 0
VIX_model <- fit$par[1] + fit$par[4]
distance <- VIX - VIX_model
distance

#re-calibrate the VIX_model
VIX_model
VIX_model2 <- VIX_sim - distance

delta_ratio <- VIX_model2/VIX_model

alpha.adjusted <- alpha*c(delta_ratio, 1, 1, delta_ratio)

#memory allocation
IV.adjusted <- matrix(NA, nrow = length(moneyness), ncol = length(maturity))

#Creating a surface 
for(i in 1:length(moneyness)){
  for(j in 1:length(maturity)){
    IV[i,j] <- f_sig(alpha.adjusted, moneyness[i], maturity[j])
  }
}
#Try plotting now
plot_ly(x =~ moneyness, y =~ maturity, z =~ IV) %>% add_surface() 


```


\newpage
# Full Approach 

1. Filter the volatility clustering of the log-returns of the underlying using a GARCH(1,1) model with Normal innovations. Use the residuals as invariants. 
2. Take and AR(1) model for the log-returns of the VIX. Use the residuals as invariants.
3. Use normal marginals for the invariants and a normal copula.
4. Generate draws for the invariants, compute next week (five days) values and reprice the portfolio.
5. Compute the VaR95 and ES95.

**Log returns of the underlying** 

```{r, message=FALSE}
# load reqruired libraries 
library("PerformanceAnalytics")

# calculate returns
sp500_rets <- PerformanceAnalytics::CalculateReturns(sp500, method="log")
vix_rets <- PerformanceAnalytics::CalculateReturns(vix, method="log")

# remove first return 
sp500_rets <- sp500_rets[-1] 
vix_rets <- vix_rets[-1]

# remove nas 
sp500_rets[is.na(sp500_rets)] <- 0
vix_rets[is.na(vix_rets)] <- 0

# display
head(sp500_rets) 
head(vix_rets)
```

### GARCH(1,1) Model 

**Model specification**

$$
\begin{aligned} 
y_{t} &= \epsilon_{t} \sigma_{t}, \\ 
\sigma^{2}_{t} &= \omega \; + \; \alpha y_{t-1}^2 + \beta \sigma_{t-1}^{2} \\  
\epsilon_{t} &\overset{i.i.d.}{\sim} \mathcal{N}(0,1), 
\end{aligned}
$$
**Mean and variance**

$$
\begin{aligned}
\mathbb{E}[Y_{t}] &\approx 0 \\ 
\mathbb{V}ar[Y_{t}] &= \mathbb{E}[\epsilon^{2}_{t}] = \mathbb{E}[\sigma^{2}_{t}] = \dfrac{\omega}{(1 - \alpha - \beta)}
\end{aligned}
$$

**Stationarity Conditions**

$$
\begin{aligned}
\omega &\geq 0 \\ 
\alpha\;,\; \beta &> 0 \\
\alpha + \beta &< 1 \;\;\;\text{(Covariance-Stationary)} 
\end{aligned}
$$

**VaR**

$$
VaR_Y(\alpha) = \Phi^{-1}(1 - \gamma)\sigma_{t}, 
$$

**Log-likelihood**

$$
\ln L(\theta|\mathbf{y})
=
- \dfrac{T}{2} \ln (2\pi) - \sum_{t=1}^{T}\ln{\sigma^{2}_{t}} - \dfrac{1}{2}\sum_{t=1}^{T} \dfrac{y_t^2}{\sigma^{2}_{t}}.
$$

## Volatility clustering of the log-returns of the underlying with GARCH(1,1) 


Which indicates a high level of autocorrelation in the returns. 

**Fitting the GARCH(1,1)**

```{r}
# source code for garch 
source(here("code", "GARCH.R")) # GARCH model implementation

# Estimate the GARCH(1,1) model
fit_garch <- f_optim_garch(sp500_rets)
```

```{r}
## Aside: If we had used the MSGARCH package 

# load MSGARCH
library("MSGARCH")
# GARCH with NOrmal innovations
garch_n <- MSGARCH::CreateSpec(variance.spec = list(model = c("sGARCH")),
                               distribution.spec = list(distribution = c("norm")))
fit_garch_n <- MSGARCH::FitML(spec = garch_n, data = sp500_rets)

#check the fit 
summary(fit_garch_n)
```


**Inpect the parameters**

```{r}
# extract parameters (omega, alpha, beta)
theta_hat_garch <- fit_garch$theta_hat
theta_hat_garch
```

**Verify stationarity**

```{r}
# make sure stationarity is satisfied 
sum(theta_hat_garch[2:3])
```

**Mean Squared Error** 
```{r}
# MSE ? 
sqrt(theta_hat_garch[1] / (1 - sum(theta_hat_garch[2:3]))) * sqrt(250)

# sd of returns annualized?
sd(sp500_rets) * sqrt(250)
```
***Residuals***

The residuals are given by: 

$$\hat{\epsilon}_t = \dfrac{y_t}{\hat{\sigma}_t}$$

```{r}
# extrct the residuals
sp500_resids <- fit_garch$eps_hat

# inspect their mean and variance 
mean(sp500_resids)
sd(sp500_resids)
```
```{r}
# Look at dependence in the residuals
par(mfrow = c(2,2))

# Eps_hat = Innovations Series
plot(sp500_resids, pch = 20)

# autocorr of innovations 
acf(sp500_resids)

# autocorr of the absolute values 
acf(abs(sp500_resids))

# autocorr of the variance of the innovations 
acf(sp500_resids^2)
```

## Fitting GARCH(1,1) with mean 

## AR(1) for the log-returns of the VIX

**First-order Autoregressive Process AR(1)**

- Let $\{\varepsilon_t\}$ be a mean-zero white noise process with variance $\sigma^{2}$. 
- Consider a process $\{X_{t}\}$, independent of $\{\varepsilon_t\}$. 
- Let $\phi$ be constant. 

The \textbf{AR(1) process} satisfies:

$$
X_{t} = \phi X_{t-1} + \varepsilon_{t}
$$

It can be shown that: 

$$
\mu_X(t) = \mathbb{E}[X_t] = \phi\mu_X(t-1) = 0 \;\;\;\;,\forall t
$$
when the process is stationary, and the autocovariance function $\gamma_X(h)$ with alg $h$ and autocorrelation $\rho_X(h)$ are given by

$$
\gamma_X(h) = \dfrac{\phi^{|h|}\sigma^2}{1 - \phi^2} \;\;\;\;\text{and}\;\;\;\; \rho_X(h) = \phi^{|h|}
$$

**VIX log-returns** 

```{r, warning=FALSE}
library("forecast")
# Construct an AR(1) model to the vix 
vix_ar1 <- ar(vix_rets, order.max = 1) 
vix_ar1$ar # phi coefficient 
```

**Stationarity of the residuals & underlying normality** 

```{r}
# extract the residuals 
vix_resids <- vix_ar1$resid
vix_resids[1] <- 0 # first residual is NA
head(vix_resids)
```


```{r}
# comes from the forecast package
checkresiduals(vix_ar1, main="Residuals for AR(1) Model")
```

## Normal Copula with Normal Marginals for the Invariants 

### Bivariate Gaussian Copula

Recall that the bivariate Gaussian copula is given by: 

$$
C^{\text{Gauss}}_{\rho}(u,v) = \Phi_{\rho}(\Phi^{-1}(u), \Phi^{-1}(v)). \;\impliedby\; H(x,y) = C(F(x), G(y))
$$
$$
C^{\text{Gauss}}_{\rho}(u,v) = 
\int_{-\infty}^{\Phi^{-1}(u)}\int_{-\infty}^{\Phi^{-1}(v)}
\dfrac{1}{2\pi \sqrt{1-\rho^{2}}}
\exp\left( - \dfrac{x^2 - 2\rho xy + y^2}{2(1 - \rho^2)}  \right)
dx dy
$$

### Gaussian marginals to the invariants 

```{r}
# invariants are the residuals 
sp500_resids <- as.vector(sp500_resids)
vix_resids <- as.vector(vix_resids)

# display some values 
head(sp500_resids, 10) 
head(vix_resids, 10)
```


```{r}
library("MASS")
## Fit marginals by MLE

# Gaussian for sp500 invariants (from the GARCH(1,1))
fit1 <- suppressWarnings(
  fitdistr(x = sp500_resids, 
           densfun = dnorm,
           start = list(mean = 0, sd = 1))
  )
theta1 <- fit1$estimate #extract fitted parameters

# Gaussian for vix invariants (from the AR(1))
fit2 <- suppressWarnings(
  fitdistr(x = vix_resids, 
           densfun = dnorm,
           start = list(mean = 0, sd = 1))
  )
theta2 <- fit2$estimate # extract fitted parameters

# display parameters
theta1
theta2
```

```{r}
# Fit a Gaussian to the marginals
U1 <- pnorm(sp500_rets_vec, mean = theta1[1], sd = theta1[2]) # sp500
U2 <- pnorm(vix_rets_vec,mean = theta2[1], sd = theta2[2]) # vix
U <- cbind(U1, U2) # join into one matrix
plot(U,
     pch = 20, cex = 0.9, 
     main="Gaussian Marginals Fitted to residuals",
     xlab="Gaussian fitted to GARCH(1,1) SP500 residuals", 
     ylab="Gaussian fitted to AR(1) VIX residuals"
     )
```

### Fitting the Gaussian Copula 

```{r}
# Obtain the best rho for the Gaussian Copula
C <- normalCopula(dim = 2)
fit <- fitCopula(C, data = U, method = "ml")
fit
```

### Simulating the invariants with the Copula 


```{r}
# random seed for replication 
set.seed(69)

##############################
### Setup & Initialization ###
##############################

# Simulation parameters
n_sim = 10000 # set number of simulations
n_ahead = 5 # days ahead to produce samples 

# preallocate matrices to store simulations
sim_inv_sp500 <- matrix(NA, nrow = n_sim, ncol=5)
sim_inv_vix <- matrix(NA, nrow = n_sim, ncol=5)

# assign days ahead 
colnames(sim_inv_sp500) <- c("T+1", "T+2", "T+3", "T+4", "T+5")
colnames(sim_inv_vix) <- c("T+1", "T+2", "T+3", "T+4", "T+5")

##############################
### Running the simulation ###
##############################

# perform n_head days of n_sim scenarios
for(t in 1:n_ahead){

  # Sample n_sim scenarios from Gaussian Copula
  U_sim <- rCopula(n_sim, fit@copula)
  
  # use copula U_sim to reproduce the marginals quantiles F^{-1}(u) with Gaussian distr
  inv1_sim <- qnorm(U_sim[,1], mean = theta1[1], sd = theta1[2]) # sp500
  inv2_sim <- qnorm(U_sim[,2], mean = theta2[1], sd = theta2[2]) # vix
  invs_sim <- cbind(rets1_sim, rets2_sim)
  
  # store simulation of log return in matrix
  sim_inv_sp500[ ,t] <- inv1_sim
  sim_inv_vix[ ,t] <- inv2_sim
}

# preview of simulated invariants
head(sim_inv_sp500)
head(sim_inv_vix)
```


### Transforming back the invariants to returns

**From GARCH(1,1) residuals to SP500 returns** 

$$
\hat{\epsilon}_t = \dfrac{y_t}{\hat{\sigma}_t}
\implies 
\hat{y_t} = \hat{\epsilon}_t\hat{\sigma}_t
$$

and 

$$
\begin{cases}
\sigma^{2}_{t} = \omega \; + \; \alpha y_{t-1}^2 + \beta \sigma_{t-1}^{2} \\ 
\hat{y_t} = \hat{\epsilon}_t\hat{\sigma}_t \\ 
\end{cases}
$$

$$
\begin{cases}
\sigma^{2}_{T+1} = \omega \; + \; \alpha y_{T}^2 + \beta \sigma_{T}^{2} \\ 
\hat{y_{T+1}} = \hat{\epsilon}_{T+1}\hat{\sigma}_{T+1} \\ 
\vdots \\ 
\sigma^{2}_{T+t} = \omega \; + \; \alpha y_{T+t-1}^2 + \beta \sigma_{T+t-1}^{2} \\ 
\hat{y_{T+t}} = \hat{\epsilon}_{T+t}\cdot\hat{\sigma}_{T+t} \\ 
\end{cases}
$$

First, obtain last conditional variance available (up to time T): 


```{r}
# load source code with GARCh custom functions
source(here("code", "GARCH.R")) # display the pdf through a 3-d chart

# data from up to T 
y <- sp500_rets_vec
sig2 <- fit_garch$sig2_hat # vector of sig2 from GARCH 
theta <- fit_garch$theta_hat # GARCH parameters 

# initial parameters
y_prev <- y[length(y)] # last sp500 observation
sig2_prev <- sig2[length(sig2)] # last sig2_T

# residuals forecasted from copula (invariants)
garch_resids_next <- sim_inv_sp500
resids_next <- garch_resids_next[1, ] # example vector of residuals for prediction 

# obtain 5days-ahead prediction for variance
sig2_forecast <- f_forecast_y(theta = theta,
                              sig2_prev = sig2_prev,
                              y_prev = y_prev, 
                              resids_next = resids_next)

sig2_forecast
```

```{r}
# apply to all rows and pack into a matrix 
sp500_sim_rets_full <- t(apply(sim_inv_sp500, 1, function(x){f_forecast_y(theta=theta, 
                                                                         sig2_prev = sig2_prev, 
                                                                         y_prev = y_prev, 
                                                                         resids_next = x)$y_next}))
colnames(sp500_sim_rets_full) <- c("T+1", "T+2", "T+3", "T+4", "T+5")
head(sp500_sim_rets_full)
```


```{r}
# example 5-days ahead simulation vs actual values: 
hist(sp500_rets, nclass=30)
abline(v=sig2_forecast$y_next, col="blue")
legend(x="topleft", 
       legend = c("Copula sim. rets"),
       col = c("blue"), 
       lwd=rep(2, time=2))
```


**From AR(1) residuals to VIX observations** 

The AR(1) model specifies 

$$
X_{t} = \phi X_{t-1} + \varepsilon_{t} \\
$$
therefore for the step ahead predictions

$$
\begin{cases}
x_{T+1} = \phi x_{T} + \varepsilon_{T} \\
x_{T+2} = \phi x_{T+1} + \varepsilon_{T+1} \\
\vdots \\ 
x_{T+t} = \phi x_{T+t-1} + \varepsilon_{T+t-1}
\end{cases}
$$




### Transforming the  simulated returns into SP500 prices and VIX values 



```{r}
# data from up to T (VIX)
x <- vix_rets_vec

# initial parameters
phi <- vix_ar1$ar
x_prev <- x[length(x)] # last vix observation

# residuals forecasted from copula (vix invariants)
ar1_resids_next <- sim_inv_vix
ar1_res_next <- ar1_resids_next[1, ] # example vector

# forecast the vix values using the copula simulated residuals 
ex_vix_forecast <- f_forecast_x(phi=phi, x_prev = x_prev, resids_next = ar1_res_next)
ex_vix_forecast
```


```{r}
# apply to all rows and pack into a matrix 
vix_sim_rets_full <- t(apply(sim_inv_vix, 1, function(x){f_forecast_x(phi=phi,
                                                                 x_prev = x_prev, 
                                                                 resids_next = x)}))
colnames(vix_sim_rets_full) <- c("T+1", "T+2", "T+3", "T+4", "T+5")
head(vix_sim_rets_full)
```


```{r}
# example 5-days ahead simulation vs actual values: 
hist(vix_rets, nclass=40)
abline(v=ex_vix_forecast, col="blue") # one simulation
legend(x="topleft", 
       legend = c("Copula VIX sim. rets."),
       col = c("blue"), 
       lwd=rep(2, time=2))
```


### Transforming returns back to SP500 prices and VIX values 

```{r}
# Obtain Initial values (last value of simulation) 
spT <- sp500[length(sp500)][[1]]
vixT <- vix[length(vix)][[1]]

# calculate the price and values from the simulated log-returns 
sim_val_mats_full <- f_logret_to_price(sp_init = spT,
                                      vix_init = vixT, 
                                      sim_rets_sp500 = sp500_sim_rets_full, 
                                      sim_rets_vix = vix_sim_rets_full
                                      )

# unpack matrices 
sp500_sim_price_full <- sim_val_mats_full$sp500 
vix_sim_vol_full <- sim_val_mats_full$vix
```


```{r}
# compare simulated returns with the price
head(sp500_sim_rets_full)
head(sp500_sim_price_full) 
```

```{r}
# compare simualted log rets with volatility 
head(vix_sim_rets_full)
head(vix_sim_vol_full)
```


### Pricing the simulation scenarios 

Recall the initial (call) options: 

1. **1x** strike $K=1600$ with maturity $T=20d$ 
2. **1x** strike $K=1605$ with maturity $T=40d$
3. **1x** strike $K=1800$ with maturity $T=40d$ 

**Option Pricing of Simulated Values**

Same as before, we calculate the price of the book of options for the simulated values using the `f_opt_price_simulation()` function under `code/OptionPricing.R`: 

```{r}
# random seed for replication 
set.seed(123)

# Initialize strikes and maturities the options 
T_vec <- c(20, 20, 40,40) #  maturities 
K_vec <- c(1600, 1650, 1750, 1800) # Strikes 

# Obtain the obtion prices from simulation values 
opt_price_mats_full <- f_opt_price_simulation(sim_price_sp500 = sp500_sim_price_full, 
                                              sim_vol_vix = vix_sim_vol_full, 
                                              K_vec = K_vec, 
                                              T_vec = T_vec, 
                                              put=FALSE)

```



```{r}
# overview of dataframes
head(opt_price_mats_full$opt1)
head(opt_price_mats_full$opt2)
head(opt_price_mats_full$opt3)
head(opt_price_mats_full$opt4)
```

### Distribution of the Profit and Loss for the Book Of Options 


**Calculating the profits** 

For each of the simulated prices and resulting premiums, we want to calculate the profit generated at each simulation timestep. 
The function used is `f_pl_simulation()`, found under `code/OptionPricing.R`. 

```{r}
# Initialize strikes and maturities the options 
T_vec <- c(20, 20, 40,40) #  maturities 
K_vec <- c(1600, 1650, 1750, 1800) # Strikes 

# Compute the profits and loses for the simulation  from the simulated option premiums
PL_mats_full <- f_pl_simulation(sim_price_sp500 = sp500_sim_price_full, 
                           opt_price_mats = opt_price_mats_full, 
                           K_vec = K_vec)

```


```{r}
# display profit matrices 
head(PL_mats_full$PL1)
head(PL_mats_full$PL2)
head(PL_mats_full$PL3)
head(PL_mats_full$PL4)
```

**Distribution of Options P/L** 

Next, using all the simulated profits and losses for each of the options, we display a histogram for the distribution for each of the options, for the aggregated 5 days of simulation: 

```{r fig.height=9, fig.width=11, fig.align='center'}
# flatten the matrices 5-days ahead simulated P/L for the three options 
sim_pl_opt1_full <- as.vector(PL_mats_full$PL1)
sim_pl_opt2_full <- as.vector(PL_mats_full$PL2)
sim_pl_opt3_full <- as.vector(PL_mats_full$PL3)
sim_pl_opt4_full <- as.vector(PL_mats_full$PL4)

# Compute the 95% VaR and 95% ES
opt1_full_VaR_ES <- f_VaR_ES(sim_pl_opt1_full, alpha = 0.05)
opt2_full_VaR_ES <- f_VaR_ES(sim_pl_opt2_full, alpha = 0.05)
opt3_full_VaR_ES <- f_VaR_ES(sim_pl_opt3_full, alpha = 0.05)
opt4_full_VaR_ES <- f_VaR_ES(sim_pl_opt4_full, alpha = 0.05)

# plot the distribution for each of the options 
par(mfrow = c(2,2))

# distribution of first option 
hist(sim_pl_opt1_full, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K_vec[1], " T=", T_vec[1], " (Call)"))
lines(density(sim_pl_opt1_full), lwd=2, col="blue")
abline(v=opt1_full_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt1_full_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt1_full)

# distribution of second option 
hist(sim_pl_opt2_full, nclass = round(10 * log(n_sim)), 
     probability = TRUE, xlab=paste0("K=", K_vec[2], " T=", T_vec[2], " (Call)"))
lines(density(sim_pl_opt2_full), lwd=2, col="blue")
abline(v=opt2_full_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt2_full_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt2_full)

# distribution of third option 
hist(sim_pl_opt3_full, nclass = round(10 * log(n_sim)),
     probability = TRUE, xlab=paste0("K=", K_vec[3], " T=", T_vec[3], " (Call)"))
lines(density(sim_pl_opt3_full), lwd=2, col="blue")
abline(v=opt3_full_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt3_full_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt3_full)

# distribution of fourth option 
hist(sim_pl_opt4_full, nclass = round(10 * log(n_sim)),
     probability = TRUE, xlab=paste0("K=", K_vec[4], " T=", T_vec[4], " (Call)"))
lines(density(sim_pl_opt4_full), lwd=2, col="blue")
abline(v=opt4_full_VaR_ES$VaR, col="red") # 95% VaR
abline(v=opt4_full_VaR_ES$ES, col="black") # expected shortfall 
rug(sim_pl_opt4_full)
```







